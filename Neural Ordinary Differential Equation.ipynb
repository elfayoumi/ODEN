{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations\n",
    "## Summary\n",
    "\n",
    "NeurIPS is the largest AI conference in the world. 4,854 papers were submitted. 4 received \"Best paper\" award. This is one of them. The basic idea is that neural networks are made up of stacked layers of simple computation nodes that work together to approximate a function. If we re-frame a neural network as an \"Ordinary Differential Equation\", we can use existing ODE solvers (like Euler's method) to approximate a function. This means no discrete layers, instead the network is a continous function. No more specifying the # of layers beforehand, instead specify the desired accuracy, it will learn how to train itself within that margin of error. It's still early stages, but this could be as big a breakthrough as GANs! \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/resnet_0_viz.png\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "        <td> <img src=\"images/odenet_0_viz.png\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    " -  Left: A Residual network defines a discrete sequence of finite transformations.\n",
    " -  Right: A ODE network defines a vector field, which continuously transforms the state.\n",
    " -  Both: Circles represent evaluation locations\n",
    "\n",
    "## Demo \n",
    "An ODENet approximated this spiral function better than a Recurrent Network. \n",
    "\n",
    "![alt text](images/demon-timeseries.png)\n",
    "\n",
    "ODENet give comparable result to ResNet but cheaper in memory\n",
    "![Ode vs ResNet](images/resnet-vs-ode.png)\n",
    "\n",
    "\n",
    "## Why Does this matter? \n",
    "\n",
    "1. Faster testing time than recurrent networks, but slower training time. Perfect for low power edge computing! (precision vs speed)\n",
    "2. More accurate results for time series predictions (!!) i.e continous-time models\n",
    "3. Opens up a whole new realm of mathematics for optimizing neural networks (Diff Equation Solvers, 100+ years of theory)\n",
    "4, Compute gradients with constant memory cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network, Global Approximators\n",
    "\n",
    "From Universal Approximation Theorem, a network made of linear matrix multiplication followed by a non-linear function can approximate any arbitrary continuous function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network\n",
    "\n",
    "A solution to this was proposed by Microsoft for the 2015 ImageNet competiton (residual networks)\n",
    "- In December of  2015, Microsoft proposed \"Residual networks\" as a solution to the ImageNet Classification Competition\n",
    "- ResNets had the best accuracy in the competition\n",
    "- ResNets utilize \"skip-connections\" between layers, which increases accuracy.\n",
    "- They were able to train networks of up to 1000 layers deep while avoiding vanishing gradients (lower accuracy)\n",
    "- 6 months later, their publicatio already had more than 200 references.\n",
    "\n",
    "he residual layer is actually quite simple: add the output of the activation function to the original input to the layer. As a formula, the k+1th layer has the formula:\n",
    "\n",
    "\\begin{equation} x_{k+1} = x_{k} + F(x_{k})\\end{equation}\n",
    "\n",
    "where F is the function of the kth layer and its activation. For example, F might represent a convolutional layer with a relu activation. This simple formula is a special case of the formula:\n",
    "\n",
    "\\begin{equation} x_{k+1} = x_{k} + h F(x_k),\\end{equation}\n",
    "\n",
    "which is the formula for the Euler method for solving ordinary differential equations (ODEs) when h=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euler Expansion\n",
    "\n",
    "Consider a simplified ODE from physics: we want to model the position x of a marble. Assume we can calculate its velocity xâ€² (the derivative of position) at any position x. We know that the marble starts at rest x(0)=0 and that its velocity at time t depends on its position through the formula:\n",
    "\n",
    "\\begin{equation} \\dot{x}(t) = f(x) \\end{equation}\n",
    "\n",
    "The Euler method solves this problem by following the physical intuition: my position at a time very close to the present depends on my current velocity and position. For example, if you are travelling at a velocity of 5 meters per second, and you travel 1 second, your position changes by 5 meters. If we travel h seconds, we will have travelled 5h meters. As a formula, we said:\n",
    "\n",
    "\\begin{equation}x(t+h) = x(t) + h \\dot{x}(t),\\end{equation}\n",
    "\n",
    "but since we know\n",
    "\n",
    "\\begin{equation} \\dot{x}(t) = f(x) \\end{equation}\n",
    "\n",
    "we can rewrite this as\n",
    "\n",
    "\\begin{equation} x(t+h) = x(t) + h f(x).\\end{equation}\n",
    "\n",
    "If you squint at this formula for the Euler method, you can see it looks just like the formula for residual layers!\n",
    "\n",
    "This observation has meant three things for designing neural networks:\n",
    "\n",
    "- New neural network layers can be created through different numerical approaches to solving ODEs\n",
    "- The possibility of arbitrarily deep neural networks\n",
    "- Training of a deep network can be improved by considering the so-called stability of the underlying ODE and its numerical discretization\n",
    "\n",
    "### 2 more points\n",
    "\n",
    "- To create arbitrarily deep networks with a finite memory footprint, design neural networks based on stable ODEs and numerical discretizations.\n",
    "- Gradient descent can be viewed as applying Euler's method for solving ordinary differential equation to gradient flow.  \n",
    "\n",
    "$$ \\dot{x} = - \\nabla f(x(t)) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
