{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations\n",
    "## Summary\n",
    "\n",
    "NeurIPS is the largest AI conference in the world. 4,854 papers were submitted. 4 received \"Best paper\" award. This is one of them. The basic idea is that neural networks are made up of stacked layers of simple computation nodes that work together to approximate a function. If we re-frame a neural network as an \"Ordinary Differential Equation\", we can use existing ODE solvers (like Euler's method) to approximate a function. This means no discrete layers, instead the network is a continous function. No more specifying the # of layers beforehand, instead specify the desired accuracy, it will learn how to train itself within that margin of error. It's still early stages, but this could be as big a breakthrough as GANs! \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"images/resnet_0_viz.png\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "        <td> <img src=\"images/odenet_0_viz.png\" alt=\"Drawing\" style=\"width: 450px;\"/> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    " -  Left: A Residual network defines a discrete sequence of finite transformations.\n",
    " -  Right: A ODE network defines a vector field, which continuously transforms the state.\n",
    " -  Both: Circles represent evaluation locations\n",
    "\n",
    "## Demo \n",
    "An ODENet approximated this spiral function better than a Recurrent Network. \n",
    "\n",
    "![alt text](images/demon-timeseries.png)\n",
    "\n",
    "ODENet give comparable result to ResNet but cheaper in memory\n",
    "![Ode vs ResNet](images/resnet-vs-ode.png)\n",
    "\n",
    "\n",
    "## Why Does this matter? \n",
    "\n",
    "1. Faster testing time than recurrent networks, but slower training time. Perfect for low power edge computing! (precision vs speed)\n",
    "2. More accurate results for time series predictions (!!) i.e continous-time models\n",
    "3. Opens up a whole new realm of mathematics for optimizing neural networks (Diff Equation Solvers, 100+ years of theory)\n",
    "4, Compute gradients with constant memory cost\n",
    "\n",
    "## Resources\n",
    " - Siraj Rava github: https://github.com/llSourcell/Neural_Differential_Equations\n",
    " - torchdiffeq in github: https://github.com/rtqichen/torchdiffeq\n",
    "\n",
    "To install torchdiffeq:\n",
    "pip install git+https://github.com/rtqichen/torchdiffeq.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network, Global Approximators\n",
    "\n",
    "From Universal Approximation Theorem, a network made of linear matrix multiplication followed by a non-linear function can approximate any arbitrary continuous function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Neural Network\n",
    "\n",
    "A solution to this was proposed by Microsoft for the 2015 ImageNet competiton (residual networks)\n",
    "- In December of  2015, Microsoft proposed \"Residual networks\" as a solution to the ImageNet Classification Competition\n",
    "- ResNets had the best accuracy in the competition\n",
    "- ResNets utilize \"skip-connections\" between layers, which increases accuracy.\n",
    "- They were able to train networks of up to 1000 layers deep while avoiding vanishing gradients (lower accuracy)\n",
    "- 6 months later, their publicatio already had more than 200 references.\n",
    "\n",
    "The residual layer is actually quite simple: add the output of the activation function to the original input to the layer. As a formula, the k+1th layer has the formula:\n",
    "\n",
    "\\begin{equation} x_{k+1} = x_{k} + F(x_{k})\\end{equation}\n",
    "\n",
    "where F is the function of the kth layer and its activation. For example, F might represent a convolutional layer with a relu activation. This simple formula is a special case of the formula:\n",
    "\n",
    "\\begin{equation} x_{k+1} = x_{k} + h F(x_k),\\end{equation}\n",
    "\n",
    "which is the formula for the Euler method for solving ordinary differential equations (ODEs) when h=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euler Expansion\n",
    "\n",
    "Consider a simplified ODE from physics: we want to model the position x of a marble. Assume we can calculate its velocity xâ€² (the derivative of position) at any position x. We know that the marble starts at rest x(0)=0 and that its velocity at time t depends on its position through the formula:\n",
    "\n",
    "\\begin{equation} \\dot{x}(t) = f(x) \\end{equation}\n",
    "\n",
    "The Euler method solves this problem by following the physical intuition: my position at a time very close to the present depends on my current velocity and position. For example, if you are travelling at a velocity of 5 meters per second, and you travel 1 second, your position changes by 5 meters. If we travel h seconds, we will have travelled 5h meters. As a formula, we said:\n",
    "\n",
    "\\begin{equation}x(t+h) = x(t) + h \\dot{x}(t),\\end{equation}\n",
    "\n",
    "but since we know\n",
    "\n",
    "\\begin{equation} \\dot{x}(t) = f(x) \\end{equation}\n",
    "\n",
    "we can rewrite this as\n",
    "\n",
    "\\begin{equation} x(t+h) = x(t) + h f(x).\\end{equation}\n",
    "\n",
    "If you squint at this formula for the Euler method, you can see it looks just like the formula for residual layers!\n",
    "\n",
    "This observation has meant three things for designing neural networks:\n",
    "\n",
    "- New neural network layers can be created through different numerical approaches to solving ODEs\n",
    "- The possibility of arbitrarily deep neural networks\n",
    "- Training of a deep network can be improved by considering the so-called stability of the underlying ODE and its numerical discretization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## What Does an ODENet Look like? \n",
    "\n",
    "*An ODE is a function that usually describes the change of some system through time. In this setting, time is a continuous variable. Now imagine a neural network is that system, and time is really something more like the depth of the network. Note that there are usually a discrete number of layers in an ANN. This is a notion of continuous number of layers.*\n",
    "\n",
    "- The team didn't use Euler's method, they computed the exact ODE solution (within a small error tolerance) using adaptive solvers (faster)\n",
    "- The dynamics change smoothly with depth. You can think of this either as having weights that are a function of depth, or as having shared weights across layers but adding the depth as an extra input to f.\n",
    "- Anywhere you can put a resnet you can put an ODEnet.\n",
    "- Each ODEBlock can be used to replace a whole stack of ResBlocks.\n",
    "- In their MNIST example, each ODEBlock replaces 6 ResBlocks.\n",
    "\n",
    "### Traditional Deep Nets\n",
    "\n",
    "\n",
    "```\n",
    "h1 = f1(x)\n",
    "h2 = f2(h1)\n",
    "h3 = f3(h2)\n",
    "h4 = f3(h3)\n",
    "y  = f5(h4)\n",
    "```\n",
    "\n",
    "### ResNets\n",
    "\n",
    "```\n",
    "h1 = f1(x)  + x\n",
    "h2 = f2(h1) + h1\n",
    "h3 = f3(h2) + h2\n",
    "h4 = f4(h3) + h3\n",
    "y  = f5(h4) + h4\n",
    "```\n",
    "\n",
    "\n",
    "- Where f1, f2, etc are neural net layers.\n",
    "- The idea is that it's easier to model a small change to an almost-correct answer than to output the whole improved answer at once.\n",
    "-This looks like a primitive ODE solver (Euler's method) that solves the trajectory of a system by just taking small steps in the direction of the system dynamics and adding them up.\n",
    "-They connection allows for better training methods.\n",
    "- What if we define a deep net as a continuously evolving system? \n",
    "- Instead of updating the hidden units layer by layer, we define their derivative with respect to depth instead\n",
    "- We can use off-the-shelf adaptive ODE solvers to compute the final state of these dynamics, and call that the output of the neural network. \n",
    "\n",
    "\n",
    "## The Adjoint Method\n",
    "\n",
    "- This approach computes gradients by solving a second, augmented ODE backwards in time, and is applicable to all ODE solvers. \n",
    "- This approach scales linearly with problem size, has low memory cost, and explicitly controls numerical error.\n",
    "- The adjoint captures how the loss function L changes with respect to the hidden state.\n",
    "- Starting from the output of the network, we can recompute the hidden state backwards in time together with the adjoint.\n",
    "\n",
    "\n",
    "![alt text](images/NODEs-Fig-2.jpeg?w=480)\n",
    "\n",
    "- A third integral then tells us how the loss changes with the parameters \\theta ( dL/d\\theta).\n",
    "- All three of these integrals can be computed in a single call to an ODE solver, which concatenates the original state, the adjoint, and the other partial derivatives into a single vector. \n",
    "- Algorithm 1 shows how to construct the necessary dynamics, and call an ODE solver to compute all gradients at once.\n",
    "\n",
    "![alt text](images/NODEs-Alg-1.jpeg?w=640)\n",
    "\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- To create arbitrarily deep networks with a finite memory footprint, design neural networks based on stable ODEs and numerical discretizations.\n",
    "- Create ODEFunc class to calculate the dervivative\n",
    "- Call ODENet black box function inside a ODEBlock class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib\n",
    "#matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 2000\n",
    "lr = 0.01\n",
    "gpu = 0\n",
    "train_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spiral2d(nspiral=1000,\n",
    "                      ntotal=500,\n",
    "                      nsample=100,\n",
    "                      start=0.,\n",
    "                      stop=1,  # approximately equal to 6pi\n",
    "                      noise_std=.1,\n",
    "                      a=0.,\n",
    "                      b=1.,\n",
    "                      savefig=True):\n",
    "    \"\"\"Parametric formula for 2d spiral is `r = a + b * theta`.\n",
    "    Args:\n",
    "      nspiral: number of spirals, i.e. batch dimension\n",
    "      ntotal: total number of datapoints per spiral\n",
    "      nsample: number of sampled datapoints for model fitting per spiral\n",
    "      start: spiral starting theta value\n",
    "      stop: spiral ending theta value\n",
    "      noise_std: observation noise standard deviation\n",
    "      a, b: parameters of the Archimedean spiral\n",
    "      savefig: plot the ground truth for sanity check\n",
    "    Returns: \n",
    "      Tuple where first element is true trajectory of size (nspiral, ntotal, 2),\n",
    "      second element is noisy observations of size (nspiral, nsample, 2),\n",
    "      third element is timestamps of size (ntotal,),\n",
    "      and fourth element is timestamps of size (nsample,)\n",
    "    \"\"\"\n",
    "\n",
    "    # add 1 all timestamps to avoid division by 0\n",
    "    orig_ts = np.linspace(start, stop, num=ntotal)\n",
    "    samp_ts = orig_ts[:nsample]\n",
    "\n",
    "    # generate clock-wise and counter clock-wise spirals in observation space\n",
    "    # with two sets of time-invariant latent dynamics\n",
    "    zs_cw = stop + 1. - orig_ts\n",
    "    rs_cw = a + b * 50. / zs_cw\n",
    "    xs, ys = rs_cw * np.cos(zs_cw) - 5., rs_cw * np.sin(zs_cw)\n",
    "    orig_traj_cw = np.stack((xs, ys), axis=1)\n",
    "\n",
    "    zs_cc = orig_ts\n",
    "    rw_cc = a + b * zs_cc\n",
    "    xs, ys = rw_cc * np.cos(zs_cc) + 5., rw_cc * np.sin(zs_cc)\n",
    "    orig_traj_cc = np.stack((xs, ys), axis=1)\n",
    "\n",
    "    if savefig:\n",
    "        plt.figure()\n",
    "        plt.plot(orig_traj_cw[:, 0], orig_traj_cw[:, 1], label='clock')\n",
    "        plt.plot(orig_traj_cc[:, 0], orig_traj_cc[:, 1], label='counter clock')\n",
    "        plt.legend()\n",
    "        plt.savefig('./ground_truth.png', dpi=500)\n",
    "        print('Saved ground truth spiral at {}'.format('./ground_truth.png'))\n",
    "\n",
    "    # sample starting timestamps\n",
    "    orig_trajs = []\n",
    "    samp_trajs = []\n",
    "    for _ in range(nspiral):\n",
    "        # don't sample t0 very near the start or the end\n",
    "        t0_idx = npr.multinomial(\n",
    "            1, [1. / (ntotal - 2. * nsample)] * (ntotal - int(2 * nsample)))\n",
    "        t0_idx = np.argmax(t0_idx) + nsample\n",
    "\n",
    "        cc = bool(npr.rand() > .5)  # uniformly select rotation\n",
    "        orig_traj = orig_traj_cc if cc else orig_traj_cw\n",
    "        orig_trajs.append(orig_traj)\n",
    "\n",
    "        samp_traj = orig_traj[t0_idx:t0_idx + nsample, :].copy()\n",
    "        samp_traj += npr.randn(*samp_traj.shape) * noise_std\n",
    "        samp_trajs.append(samp_traj)\n",
    "\n",
    "    # batching for sample trajectories is good for RNN; batching for original\n",
    "    # trajectories only for ease of indexing\n",
    "    orig_trajs = np.stack(orig_trajs, axis=0)\n",
    "    samp_trajs = np.stack(samp_trajs, axis=0)\n",
    "\n",
    "    return orig_trajs, samp_trajs, orig_ts, samp_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODE Function\n",
    "This is the function that corresponds to derivative, create a block that you need to integrate like in ResNet (ResNet Block you keep repeating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, nhidden=20):\n",
    "        super(LatentODEfunc, self).__init__()\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, nhidden)\n",
    "        self.fc3 = nn.Linear(nhidden, latent_dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.fc1(x)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.elu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecognitionRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=2, nhidden=25, nbatch=1):\n",
    "        super(RecognitionRNN, self).__init__()\n",
    "        self.nhidden = nhidden\n",
    "        self.nbatch = nbatch\n",
    "        self.i2h = nn.Linear(obs_dim + nhidden, nhidden)\n",
    "        self.h2o = nn.Linear(nhidden, latent_dim * 2)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        combined = torch.cat((x, h), dim=1)\n",
    "        h = torch.tanh(self.i2h(combined))\n",
    "        out = self.h2o(h)\n",
    "        return out, h\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.nbatch, self.nhidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=2, nhidden=20):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc1(z)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(x, mean, logvar):\n",
    "    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device)\n",
    "    const = torch.log(const)\n",
    "    return -.5 * (const + logvar + (x - mean) ** 2. / torch.exp(logvar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder (VAE)\n",
    "Variational autoencoders are a slightly more modern and interesting take on autoencoding.\n",
    "\n",
    "What is a variational autoencoder, you ask? It's a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \"generative model\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s define some notions:\n",
    "1. $X$: data that we want to model a.k.a the animal\n",
    "2. $z$: latent variable a.k.a our imagination\n",
    "3. $p(X)$: probability density of the data, i.e. that animal kingdom\n",
    "4. $p(z)$: probability density of latent variable, i.e. our brain, the source of our imagination\n",
    "5. $p(X|z)$: probability density of generating data given latent variable, e.g. turning imagination into real animal\n",
    "\n",
    "\n",
    "Our objective here is to model the data, hence we want to find $p(X)$ Using the law of probability, we could find it in relation with $z$ as follows:\n",
    "<h1><center>$p(X) = \\int p(X|z)p(z)dz$</center></h1>\n",
    "that is, we marginalize out $z$ from the joint probability distribution $P(X,z)$.\n",
    "\n",
    "The idea of VAE is to infer $p(z)$ using $p(z|X)$. This is make a lot of sense if we think about it: we want to make our latent variable likely under our data. Talking in term of our fable example, we want to limit our imagination only on animal kingdom domain, so we shouldnâ€™t imagine about things like root, leaf, tyre, glass, GPU, refrigerator, doormat, â€¦ as itâ€™s unlikely that those things have anything to do with things that come from the animal kingdom. Right?\n",
    "\n",
    "But the problem is, we have to infer that density  $p(z|X)$, as we donâ€™t know it yet. In VAE, as it name suggests, we infer $p(z|X)$ using a method called Variational Inference (VI). VI is one of the popular choice of method in bayesian inference, the other one being MCMC method. The main idea of VI is to pose the inference by approach it as an optimization problem. How? By modeling the true density $p(z|X)$ using simpler density that is easy to evaluate, e.g. Gaussian, and minimize the difference between those two density using KL divergence metric, which tells us how difference it is $p$ and $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullbackâ€“Leibler divergence\n",
    "Alright, now letâ€™s say we want to infer $p(z|X)$ using $q(z|X)$. The KL divergence then formulated as follows:\n",
    "<h1><center>$D_{KL}[q[z|X)||p(z|X)] = \\sum_z q(z|X) \\log \\frac{q(z|X)}{p(z|X)} = E_q[\\log q(z|X) - \\log p(z|X)]$</center></h1>\n",
    "# What KL-Divergence do?\n",
    "* allows use to compare 2 probability distributions\n",
    "* if $q$ is the same as  $p$, then KLD = 0\n",
    "* if $q$ is different from  $p$, KLD > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cost function\n",
    "The cost function we want to minimise is the negative of the evidence lower bound: 'ELBO':\n",
    "<h1><center>$ELBO = E[\\log p(X|z)] - D_{KL} [q(z|X) ||p(z)]$</center></h1>\n",
    "\n",
    "The first term is $E_q[\\log p(X|z)]$ is the cross entropy we know. \n",
    "The second term is the KL divergence between the prior $p(z)$ and our latent model. We can choose the prior density any density we want, but it is much simpler to use normal density function: $p(z) = \\mathcal{N}(0,1)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL distribution between two normal distribution\n",
    "I am using book: The Matrix Cookbook from: http://coin.wne.uw.edu.pl/pbiernacki/matrix_cookbook.pdf\n",
    "1. $q(z|X) = \\mathcal{N}(\\mu_1, \\Sigma_1^2)$\n",
    "2. $p(z) = \\mathcal{N}(\\mu_2, \\Sigma_2^2)$\n",
    "3. $D_{KL}[q(z|X)||p(z:X)] = \\int p(z) [\\log p(z) - \\log q(z|X)] dz$\n",
    "4. $D_{KL}[\\mathcal{N}(\\mu_1, \\Sigma_1)||\\mathcal{N}(\\mu_2,\\Sigma_2)]= \\frac{1}{2} [\\log \\frac{|\\Sigma_2|}{|\\Sigma_1|} - d  + tr{\\Sigma_2^{-1}\\Sigma_1} + (\\mu_2-\\mu_1)^T\\Sigma_2^{-1}(\\mu_2-\\mu_1)]$\n",
    "4. $d$ is the dimension of the latent vector.\n",
    "\n",
    "If we have $d=1$, and $p(x) = \\mathcal{N}(0,1)$ then the formula becomes very simple.\n",
    "$KL [\\mathcal{N}(\\mu, \\Sigma^2) || \\mathcal{N}(0,1)  ] = \\frac{1}{2}(\\log(\\Sigma) -1  + \\Sigma + \\mu^2)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLD(N(3,2), N(0,1)) = 5.306853 , value =  5.346573590279973\n",
      "KLD(N(3,1), N(2.9,1)) = 0.0049999906\n",
      "KLD(N(3,1), N(3,1)) = 0.0\n",
      "KLD(MVN1, MVN2)  =  5.98301584426063\n"
     ]
    }
   ],
   "source": [
    "#tensor flow allows you to get the KL between two distribution...\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.distributions as dis\n",
    "Normal = tf.contrib.distributions.Normal\n",
    "t = dis.kl_divergence(Normal(3.0, 2.0), Normal(0.0, 1.0))\n",
    "t2  =  dis.kl_divergence(Normal(3.0, 1.0), Normal(2.9,1.0))\n",
    "t3 =  dis.kl_divergence(Normal(3.0, 1.0), Normal(3.0,1.0))\n",
    "\n",
    "#Multivariate Normal distribution\n",
    "mu1 =np.array( [1.,2])\n",
    "cov1=np.array([[3,3/5], [3/5,2]])\n",
    "\n",
    "\n",
    "mu2 = np.array([4,2.1])\n",
    "cov2= np.array([[1,3/5], [3/5,2]])\n",
    "\n",
    "tmvn = dis.kl_divergence(tf.contrib.distributions.MultivariateNormalFullCovariance(loc=mu1,covariance_matrix=cov1),\n",
    "        tf.contrib.distributions.MultivariateNormalFullCovariance(\n",
    "           loc=mu2,\n",
    "           covariance_matrix=cov2) )\n",
    "\n",
    "\n",
    "with tf.Session() as session:\n",
    "    t_val = session.run(t)\n",
    "    print ('KLD(N(3,2), N(0,1)) =', t_val, \", value = \", .5*(np.log(2)  - 1  + 2.0 + 3**2 ))\n",
    "    t_val = session.run(t2)\n",
    "    print('KLD(N(3,1), N(2.9,1)) =', t_val)\n",
    "    t_val = session.run(t3)\n",
    "    print('KLD(N(3,1), N(3,1)) =', t_val)\n",
    "    t_val =session.run(tmvn)\n",
    "    print('KLD(MVN1, MVN2)  = ', t_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D_{KL}[\\mathcal{N}(\\mu_1, \\Sigma_1)||\\mathcal{N}(\\mu_2,\\Sigma_2)]= \\frac{1}{2} [\\log \\frac{|\\Sigma_2|}{|\\Sigma_1|} - d  + tr{\\Sigma_2^{-1}\\Sigma_1} + (\\mu_2-\\mu_1)^T\\Sigma_2^{-1}(\\mu_2-\\mu_1)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.98301584426063\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dkl = 0.5*(np.log(np.linalg.det(cov2)/np.linalg.det(cov1) )- 2 + \n",
    "           np.trace(np.linalg.inv(cov2)@cov1) + np.transpose(mu2-mu1)@np.linalg.inv(cov2)@(mu2-mu1))\n",
    "print(dkl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_kl(mu1, lv1, mu2, lv2):\n",
    "    v1 = torch.exp(lv1)\n",
    "    v2 = torch.exp(lv2)\n",
    "    lstd1 = lv1 / 2.\n",
    "    lstd2 = lv2 / 2.\n",
    "\n",
    "    kl = lstd2 - lstd1 + ((v1 + (mu1 - mu2) ** 2.) / (2. * v2)) - .5\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ground truth spiral at ./ground_truth.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "latent_dim = 4\n",
    "nhidden = 20\n",
    "rnn_nhidden = 25\n",
    "obs_dim = 2\n",
    "nspiral = 1000\n",
    "start = 0.\n",
    "stop = 6 * np.pi\n",
    "noise_std = .3\n",
    "a = 0.\n",
    "b = .3\n",
    "ntotal = 1000\n",
    "nsample = 100\n",
    "device = torch.device('cuda:' + str(gpu)\n",
    "                      if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# generate toy spiral data\n",
    "orig_trajs, samp_trajs, orig_ts, samp_ts = generate_spiral2d(\n",
    "    nspiral=nspiral,\n",
    "    start=start,\n",
    "    stop=stop,\n",
    "    noise_std=noise_std,\n",
    "    a=a, b=b\n",
    ")\n",
    "orig_trajs = torch.from_numpy(orig_trajs).float().to(device)\n",
    "samp_trajs = torch.from_numpy(samp_trajs).float().to(device)\n",
    "samp_ts = torch.from_numpy(samp_ts).float().to(device)\n",
    "\n",
    "# model\n",
    "func = LatentODEfunc(latent_dim, nhidden).to(device)\n",
    "rec = RecognitionRNN(latent_dim, obs_dim, rnn_nhidden, nspiral).to(device)\n",
    "dec = Decoder(latent_dim, obs_dim, nhidden).to(device)\n",
    "params = (list(func.parameters()) + list(dec.parameters()) + list(rec.parameters()))\n",
    "optimizer = optim.Adam(params, lr=lr)\n",
    "loss_meter = RunningAverageMeter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_dir is not None:\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    ckpt_path = os.path.join(train_dir, 'ckpt.pth')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        func.load_state_dict(checkpoint['func_state_dict'])\n",
    "        rec.load_state_dict(checkpoint['rec_state_dict'])\n",
    "        dec.load_state_dict(checkpoint['dec_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        orig_trajs = checkpoint['orig_trajs']\n",
    "        samp_trajs = checkpoint['samp_trajs']\n",
    "        orig_ts = checkpoint['orig_ts']\n",
    "        samp_ts = checkpoint['samp_ts']\n",
    "        print('Loaded ckpt from {}'.format(ckpt_path))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 1, running avg elbo: -19193.5117\n",
      "Iter: 2, running avg elbo: -19187.1266\n",
      "Iter: 3, running avg elbo: -19172.0962\n",
      "Iter: 4, running avg elbo: -19144.0185\n",
      "Iter: 5, running avg elbo: -19097.5509\n",
      "Iter: 6, running avg elbo: -19028.1374\n",
      "Iter: 7, running avg elbo: -18936.9645\n",
      "Iter: 8, running avg elbo: -18846.5738\n",
      "Iter: 9, running avg elbo: -18753.8660\n",
      "Iter: 10, running avg elbo: -18649.3661\n",
      "Iter: 11, running avg elbo: -18539.8010\n",
      "Iter: 12, running avg elbo: -18431.0176\n",
      "Iter: 13, running avg elbo: -18323.3159\n",
      "Iter: 14, running avg elbo: -18216.7535\n",
      "Iter: 15, running avg elbo: -18108.8497\n",
      "Iter: 16, running avg elbo: -17997.6967\n",
      "Iter: 17, running avg elbo: -17884.4504\n",
      "Iter: 18, running avg elbo: -17769.6621\n",
      "Iter: 19, running avg elbo: -17654.9847\n",
      "Iter: 20, running avg elbo: -17541.5794\n",
      "Iter: 21, running avg elbo: -17428.3898\n",
      "Iter: 22, running avg elbo: -17315.4519\n",
      "Iter: 23, running avg elbo: -17201.8896\n",
      "Iter: 24, running avg elbo: -17087.8663\n",
      "Iter: 25, running avg elbo: -16975.5271\n",
      "Iter: 26, running avg elbo: -16864.9131\n",
      "Iter: 27, running avg elbo: -16755.1221\n",
      "Iter: 28, running avg elbo: -16646.0512\n",
      "Iter: 29, running avg elbo: -16537.6632\n",
      "Iter: 30, running avg elbo: -16429.7152\n",
      "Iter: 31, running avg elbo: -16322.3396\n",
      "Iter: 32, running avg elbo: -16216.2670\n",
      "Iter: 33, running avg elbo: -16111.7578\n",
      "Iter: 34, running avg elbo: -16007.7060\n",
      "Iter: 35, running avg elbo: -15903.8731\n",
      "Iter: 36, running avg elbo: -15800.7332\n",
      "Iter: 37, running avg elbo: -15698.9482\n",
      "Iter: 38, running avg elbo: -15598.0658\n",
      "Iter: 39, running avg elbo: -15497.7649\n",
      "Iter: 40, running avg elbo: -15398.1844\n",
      "Iter: 41, running avg elbo: -15299.1022\n",
      "Iter: 42, running avg elbo: -15200.4992\n",
      "Iter: 43, running avg elbo: -15102.9349\n",
      "Iter: 44, running avg elbo: -15005.6128\n",
      "Iter: 45, running avg elbo: -14909.1619\n",
      "Iter: 46, running avg elbo: -14812.6931\n",
      "Iter: 47, running avg elbo: -14716.7915\n",
      "Iter: 48, running avg elbo: -14621.3823\n",
      "Iter: 49, running avg elbo: -14526.5407\n",
      "Iter: 50, running avg elbo: -14431.7666\n",
      "Iter: 51, running avg elbo: -14337.2463\n",
      "Iter: 52, running avg elbo: -14243.1889\n",
      "Iter: 53, running avg elbo: -14149.5577\n",
      "Iter: 54, running avg elbo: -14055.7016\n",
      "Iter: 55, running avg elbo: -13962.2528\n",
      "Iter: 56, running avg elbo: -13869.0015\n",
      "Iter: 57, running avg elbo: -13776.9277\n",
      "Iter: 58, running avg elbo: -13684.5717\n",
      "Iter: 59, running avg elbo: -13593.1665\n",
      "Iter: 60, running avg elbo: -13501.5988\n",
      "Iter: 61, running avg elbo: -13410.3537\n",
      "Iter: 62, running avg elbo: -13319.3039\n",
      "Iter: 63, running avg elbo: -13228.6422\n",
      "Iter: 64, running avg elbo: -13138.4470\n",
      "Iter: 65, running avg elbo: -13048.2571\n",
      "Iter: 66, running avg elbo: -12957.9482\n",
      "Iter: 67, running avg elbo: -12867.5073\n",
      "Iter: 68, running avg elbo: -12777.1896\n",
      "Iter: 69, running avg elbo: -12687.4876\n",
      "Iter: 70, running avg elbo: -12598.1203\n",
      "Iter: 71, running avg elbo: -12509.1557\n",
      "Iter: 72, running avg elbo: -12419.9263\n",
      "Iter: 73, running avg elbo: -12331.1256\n",
      "Iter: 74, running avg elbo: -12242.6743\n",
      "Iter: 75, running avg elbo: -12154.6284\n",
      "Iter: 76, running avg elbo: -12067.0485\n",
      "Iter: 77, running avg elbo: -11980.0698\n",
      "Iter: 78, running avg elbo: -11893.1764\n",
      "Iter: 79, running avg elbo: -11807.0836\n",
      "Iter: 80, running avg elbo: -11721.3149\n",
      "Iter: 81, running avg elbo: -11635.5125\n",
      "Iter: 82, running avg elbo: -11550.3898\n",
      "Iter: 83, running avg elbo: -11465.6042\n",
      "Iter: 84, running avg elbo: -11381.5374\n",
      "Iter: 85, running avg elbo: -11297.5368\n",
      "Iter: 86, running avg elbo: -11213.4907\n",
      "Iter: 87, running avg elbo: -11130.4260\n",
      "Iter: 88, running avg elbo: -11047.6774\n",
      "Iter: 89, running avg elbo: -10965.1030\n",
      "Iter: 90, running avg elbo: -10883.1296\n",
      "Iter: 91, running avg elbo: -10801.9320\n",
      "Iter: 92, running avg elbo: -10720.2937\n",
      "Iter: 93, running avg elbo: -10639.3670\n",
      "Iter: 94, running avg elbo: -10558.6131\n",
      "Iter: 95, running avg elbo: -10478.0746\n",
      "Iter: 96, running avg elbo: -10398.7103\n",
      "Iter: 97, running avg elbo: -10322.1815\n",
      "Iter: 98, running avg elbo: -10243.5132\n",
      "Iter: 99, running avg elbo: -10165.6256\n",
      "Iter: 100, running avg elbo: -10087.7242\n",
      "Iter: 101, running avg elbo: -10011.2169\n",
      "Iter: 102, running avg elbo: -9933.5570\n",
      "Iter: 103, running avg elbo: -9857.9467\n",
      "Iter: 104, running avg elbo: -9781.3749\n",
      "Iter: 105, running avg elbo: -9706.4729\n",
      "Iter: 106, running avg elbo: -9630.9159\n",
      "Iter: 107, running avg elbo: -9556.9679\n",
      "Iter: 108, running avg elbo: -9482.5716\n",
      "Iter: 109, running avg elbo: -9409.4470\n",
      "Iter: 110, running avg elbo: -9336.1994\n",
      "Iter: 111, running avg elbo: -9263.7563\n",
      "Iter: 112, running avg elbo: -9191.9953\n",
      "Iter: 113, running avg elbo: -9120.2693\n",
      "Iter: 114, running avg elbo: -9049.3730\n",
      "Iter: 115, running avg elbo: -8978.3286\n",
      "Iter: 116, running avg elbo: -8908.3707\n",
      "Iter: 117, running avg elbo: -8838.9254\n",
      "Iter: 118, running avg elbo: -8769.9487\n",
      "Iter: 119, running avg elbo: -8701.4296\n",
      "Iter: 120, running avg elbo: -8633.2425\n",
      "Iter: 121, running avg elbo: -8565.1915\n",
      "Iter: 122, running avg elbo: -8498.3407\n",
      "Iter: 123, running avg elbo: -8431.9045\n",
      "Iter: 124, running avg elbo: -8366.1071\n",
      "Iter: 125, running avg elbo: -8300.6691\n",
      "Iter: 126, running avg elbo: -8235.6310\n",
      "Iter: 127, running avg elbo: -8171.3089\n",
      "Iter: 128, running avg elbo: -8107.1181\n",
      "Iter: 129, running avg elbo: -8043.6315\n",
      "Iter: 130, running avg elbo: -7980.3541\n",
      "Iter: 131, running avg elbo: -7917.7115\n",
      "Iter: 132, running avg elbo: -7855.4146\n",
      "Iter: 133, running avg elbo: -7793.5624\n",
      "Iter: 134, running avg elbo: -7732.5215\n",
      "Iter: 135, running avg elbo: -7671.4190\n",
      "Iter: 136, running avg elbo: -7610.8106\n",
      "Iter: 137, running avg elbo: -7550.7214\n",
      "Iter: 138, running avg elbo: -7491.4324\n",
      "Iter: 139, running avg elbo: -7432.4970\n",
      "Iter: 140, running avg elbo: -7374.1049\n",
      "Iter: 141, running avg elbo: -7316.3340\n",
      "Iter: 142, running avg elbo: -7258.5002\n",
      "Iter: 143, running avg elbo: -7201.1226\n",
      "Iter: 144, running avg elbo: -7143.9228\n",
      "Iter: 145, running avg elbo: -7087.5482\n",
      "Iter: 146, running avg elbo: -7031.7079\n",
      "Iter: 147, running avg elbo: -6975.9069\n",
      "Iter: 148, running avg elbo: -6920.3643\n",
      "Iter: 149, running avg elbo: -6865.6544\n",
      "Iter: 150, running avg elbo: -6811.4185\n",
      "Iter: 151, running avg elbo: -6758.6332\n",
      "Iter: 152, running avg elbo: -6705.6817\n",
      "Iter: 153, running avg elbo: -6652.6816\n",
      "Iter: 154, running avg elbo: -6599.5305\n",
      "Iter: 155, running avg elbo: -6547.5615\n",
      "Iter: 156, running avg elbo: -6496.6523\n",
      "Iter: 157, running avg elbo: -6445.1666\n",
      "Iter: 158, running avg elbo: -6394.3955\n",
      "Iter: 159, running avg elbo: -6344.4023\n",
      "Iter: 160, running avg elbo: -6293.5802\n",
      "Iter: 161, running avg elbo: -6243.8681\n",
      "Iter: 162, running avg elbo: -6194.6385\n",
      "Iter: 163, running avg elbo: -6145.0605\n",
      "Iter: 164, running avg elbo: -6096.8933\n",
      "Iter: 165, running avg elbo: -6048.6621\n",
      "Iter: 166, running avg elbo: -6000.7779\n",
      "Iter: 167, running avg elbo: -5953.3055\n",
      "Iter: 168, running avg elbo: -5905.5949\n",
      "Iter: 169, running avg elbo: -5858.8971\n",
      "Iter: 170, running avg elbo: -5812.1471\n",
      "Iter: 171, running avg elbo: -5765.8593\n",
      "Iter: 172, running avg elbo: -5720.0670\n",
      "Iter: 173, running avg elbo: -5674.3077\n",
      "Iter: 174, running avg elbo: -5629.5082\n",
      "Iter: 175, running avg elbo: -5584.4992\n",
      "Iter: 176, running avg elbo: -5540.1899\n",
      "Iter: 177, running avg elbo: -5495.8471\n",
      "Iter: 178, running avg elbo: -5451.8611\n",
      "Iter: 179, running avg elbo: -5408.3451\n",
      "Iter: 180, running avg elbo: -5365.1559\n",
      "Iter: 181, running avg elbo: -5322.4885\n",
      "Iter: 182, running avg elbo: -5279.8646\n",
      "Iter: 183, running avg elbo: -5237.9155\n",
      "Iter: 184, running avg elbo: -5196.2510\n",
      "Iter: 185, running avg elbo: -5154.9300\n",
      "Iter: 186, running avg elbo: -5113.8670\n",
      "Iter: 187, running avg elbo: -5073.1026\n",
      "Iter: 188, running avg elbo: -5032.9616\n",
      "Iter: 189, running avg elbo: -4993.0910\n",
      "Iter: 190, running avg elbo: -4953.5859\n",
      "Iter: 191, running avg elbo: -4914.3018\n",
      "Iter: 192, running avg elbo: -4875.4029\n",
      "Iter: 193, running avg elbo: -4836.9113\n",
      "Iter: 194, running avg elbo: -4798.7420\n",
      "Iter: 195, running avg elbo: -4760.7670\n",
      "Iter: 196, running avg elbo: -4723.4237\n",
      "Iter: 197, running avg elbo: -4686.0135\n",
      "Iter: 198, running avg elbo: -4648.8927\n",
      "Iter: 199, running avg elbo: -4612.3456\n",
      "Iter: 200, running avg elbo: -4575.7948\n",
      "Iter: 201, running avg elbo: -4539.6746\n",
      "Iter: 202, running avg elbo: -4503.7508\n",
      "Iter: 203, running avg elbo: -4468.4185\n",
      "Iter: 204, running avg elbo: -4433.3792\n",
      "Iter: 205, running avg elbo: -4398.5397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 206, running avg elbo: -4363.7811\n",
      "Iter: 207, running avg elbo: -4329.4350\n",
      "Iter: 208, running avg elbo: -4295.4170\n",
      "Iter: 209, running avg elbo: -4261.7653\n",
      "Iter: 210, running avg elbo: -4228.2882\n",
      "Iter: 211, running avg elbo: -4195.1500\n",
      "Iter: 212, running avg elbo: -4162.3768\n",
      "Iter: 213, running avg elbo: -4129.8238\n",
      "Iter: 214, running avg elbo: -4097.7222\n",
      "Iter: 215, running avg elbo: -4065.8163\n",
      "Iter: 216, running avg elbo: -4034.1068\n",
      "Iter: 217, running avg elbo: -4002.7098\n",
      "Iter: 218, running avg elbo: -3971.6349\n",
      "Iter: 219, running avg elbo: -3940.7852\n",
      "Iter: 220, running avg elbo: -3910.0879\n",
      "Iter: 221, running avg elbo: -3879.8199\n",
      "Iter: 222, running avg elbo: -3849.6382\n",
      "Iter: 223, running avg elbo: -3819.7820\n",
      "Iter: 224, running avg elbo: -3790.0708\n",
      "Iter: 225, running avg elbo: -3760.7716\n",
      "Iter: 226, running avg elbo: -3731.6898\n",
      "Iter: 227, running avg elbo: -3702.9346\n",
      "Iter: 228, running avg elbo: -3674.3515\n",
      "Iter: 229, running avg elbo: -3645.9719\n",
      "Iter: 230, running avg elbo: -3617.8110\n",
      "Iter: 231, running avg elbo: -3589.9951\n",
      "Iter: 232, running avg elbo: -3562.3135\n",
      "Iter: 233, running avg elbo: -3534.8495\n",
      "Iter: 234, running avg elbo: -3507.7762\n",
      "Iter: 235, running avg elbo: -3480.6631\n",
      "Iter: 236, running avg elbo: -3453.9519\n",
      "Iter: 237, running avg elbo: -3427.4097\n",
      "Iter: 238, running avg elbo: -3400.8813\n",
      "Iter: 239, running avg elbo: -3374.8168\n",
      "Iter: 240, running avg elbo: -3348.8702\n",
      "Iter: 241, running avg elbo: -3323.2700\n",
      "Iter: 242, running avg elbo: -3297.6303\n",
      "Iter: 243, running avg elbo: -3272.4953\n",
      "Iter: 244, running avg elbo: -3247.4423\n",
      "Iter: 245, running avg elbo: -3222.6228\n",
      "Iter: 246, running avg elbo: -3197.9510\n",
      "Iter: 247, running avg elbo: -3173.4946\n",
      "Iter: 248, running avg elbo: -3149.3194\n",
      "Iter: 249, running avg elbo: -3125.3681\n",
      "Iter: 250, running avg elbo: -3101.5530\n",
      "Iter: 251, running avg elbo: -3077.9334\n",
      "Iter: 252, running avg elbo: -3054.4909\n",
      "Iter: 253, running avg elbo: -3031.6125\n",
      "Iter: 254, running avg elbo: -3008.9075\n",
      "Iter: 255, running avg elbo: -2985.8604\n",
      "Iter: 256, running avg elbo: -2963.0728\n",
      "Iter: 257, running avg elbo: -2940.9913\n",
      "Iter: 258, running avg elbo: -2919.0583\n",
      "Iter: 259, running avg elbo: -2896.8733\n",
      "Iter: 260, running avg elbo: -2874.6599\n",
      "Iter: 261, running avg elbo: -2852.8769\n",
      "Iter: 262, running avg elbo: -2831.2881\n",
      "Iter: 263, running avg elbo: -2809.6289\n",
      "Iter: 264, running avg elbo: -2788.1623\n",
      "Iter: 265, running avg elbo: -2767.0870\n",
      "Iter: 266, running avg elbo: -2746.2081\n",
      "Iter: 267, running avg elbo: -2725.3515\n",
      "Iter: 268, running avg elbo: -2704.6328\n",
      "Iter: 269, running avg elbo: -2684.0008\n",
      "Iter: 270, running avg elbo: -2663.6230\n",
      "Iter: 271, running avg elbo: -2643.4914\n",
      "Iter: 272, running avg elbo: -2623.2792\n",
      "Iter: 273, running avg elbo: -2603.3415\n",
      "Iter: 274, running avg elbo: -2583.4501\n",
      "Iter: 275, running avg elbo: -2563.7731\n",
      "Iter: 276, running avg elbo: -2544.2558\n",
      "Iter: 277, running avg elbo: -2525.0691\n",
      "Iter: 278, running avg elbo: -2505.9450\n",
      "Iter: 279, running avg elbo: -2486.8903\n",
      "Iter: 280, running avg elbo: -2467.9507\n",
      "Iter: 281, running avg elbo: -2449.1926\n",
      "Iter: 282, running avg elbo: -2430.6343\n",
      "Iter: 283, running avg elbo: -2412.5077\n",
      "Iter: 284, running avg elbo: -2394.1690\n",
      "Iter: 285, running avg elbo: -2376.1045\n",
      "Iter: 286, running avg elbo: -2358.2806\n",
      "Iter: 287, running avg elbo: -2341.0981\n",
      "Iter: 288, running avg elbo: -2324.3400\n",
      "Iter: 289, running avg elbo: -2307.6985\n",
      "Iter: 290, running avg elbo: -2290.7188\n",
      "Iter: 291, running avg elbo: -2273.4242\n",
      "Iter: 292, running avg elbo: -2256.4746\n",
      "Iter: 293, running avg elbo: -2240.0859\n",
      "Iter: 294, running avg elbo: -2223.6111\n",
      "Iter: 295, running avg elbo: -2206.9877\n",
      "Iter: 296, running avg elbo: -2190.7565\n",
      "Iter: 297, running avg elbo: -2174.4995\n",
      "Iter: 298, running avg elbo: -2158.3655\n",
      "Iter: 299, running avg elbo: -2142.4392\n",
      "Iter: 300, running avg elbo: -2126.5240\n",
      "Iter: 301, running avg elbo: -2110.5711\n",
      "Iter: 302, running avg elbo: -2094.8497\n",
      "Iter: 303, running avg elbo: -2079.1372\n",
      "Iter: 304, running avg elbo: -2063.5748\n",
      "Iter: 305, running avg elbo: -2048.0652\n",
      "Iter: 306, running avg elbo: -2032.7748\n",
      "Iter: 307, running avg elbo: -2017.4393\n",
      "Iter: 308, running avg elbo: -2002.3205\n",
      "Iter: 309, running avg elbo: -1987.3689\n",
      "Iter: 310, running avg elbo: -1972.3740\n",
      "Iter: 311, running avg elbo: -1957.4855\n",
      "Iter: 312, running avg elbo: -1942.9104\n",
      "Iter: 313, running avg elbo: -1928.4176\n",
      "Iter: 314, running avg elbo: -1914.0360\n",
      "Iter: 315, running avg elbo: -1899.7211\n",
      "Iter: 316, running avg elbo: -1885.5054\n",
      "Iter: 317, running avg elbo: -1871.4625\n",
      "Iter: 318, running avg elbo: -1857.3967\n",
      "Iter: 319, running avg elbo: -1843.4175\n",
      "Iter: 320, running avg elbo: -1829.5152\n",
      "Iter: 321, running avg elbo: -1815.8467\n",
      "Iter: 322, running avg elbo: -1802.3293\n",
      "Iter: 323, running avg elbo: -1789.0414\n",
      "Iter: 324, running avg elbo: -1776.2351\n",
      "Iter: 325, running avg elbo: -1763.7804\n",
      "Iter: 326, running avg elbo: -1751.5629\n",
      "Iter: 327, running avg elbo: -1739.0982\n",
      "Iter: 328, running avg elbo: -1726.1988\n",
      "Iter: 329, running avg elbo: -1713.5342\n",
      "Iter: 330, running avg elbo: -1701.0918\n",
      "Iter: 331, running avg elbo: -1688.8275\n",
      "Iter: 332, running avg elbo: -1676.5545\n",
      "Iter: 333, running avg elbo: -1664.3458\n",
      "Iter: 334, running avg elbo: -1652.0742\n",
      "Iter: 335, running avg elbo: -1639.8960\n",
      "Iter: 336, running avg elbo: -1627.8642\n",
      "Iter: 337, running avg elbo: -1615.8823\n",
      "Iter: 338, running avg elbo: -1603.9298\n",
      "Iter: 339, running avg elbo: -1592.2296\n",
      "Iter: 340, running avg elbo: -1580.5881\n",
      "Iter: 341, running avg elbo: -1568.9081\n",
      "Iter: 342, running avg elbo: -1557.2827\n",
      "Iter: 343, running avg elbo: -1545.8585\n",
      "Iter: 344, running avg elbo: -1534.5315\n",
      "Iter: 345, running avg elbo: -1523.3396\n",
      "Iter: 346, running avg elbo: -1512.3170\n",
      "Iter: 347, running avg elbo: -1501.3284\n",
      "Iter: 348, running avg elbo: -1490.3445\n",
      "Iter: 349, running avg elbo: -1479.3400\n",
      "Iter: 350, running avg elbo: -1468.5573\n",
      "Iter: 351, running avg elbo: -1457.7897\n",
      "Iter: 352, running avg elbo: -1447.0969\n",
      "Iter: 353, running avg elbo: -1436.5810\n",
      "Iter: 354, running avg elbo: -1426.0514\n",
      "Iter: 355, running avg elbo: -1415.5948\n",
      "Iter: 356, running avg elbo: -1405.2673\n",
      "Iter: 357, running avg elbo: -1394.9994\n",
      "Iter: 358, running avg elbo: -1384.7854\n",
      "Iter: 359, running avg elbo: -1374.6918\n",
      "Iter: 360, running avg elbo: -1364.6805\n",
      "Iter: 361, running avg elbo: -1354.8175\n",
      "Iter: 362, running avg elbo: -1345.0407\n",
      "Iter: 363, running avg elbo: -1335.3846\n",
      "Iter: 364, running avg elbo: -1325.8112\n",
      "Iter: 365, running avg elbo: -1316.4362\n",
      "Iter: 366, running avg elbo: -1307.2920\n",
      "Iter: 367, running avg elbo: -1298.2278\n",
      "Iter: 368, running avg elbo: -1289.1037\n",
      "Iter: 369, running avg elbo: -1280.1271\n",
      "Iter: 370, running avg elbo: -1270.9020\n",
      "Iter: 371, running avg elbo: -1261.8077\n",
      "Iter: 372, running avg elbo: -1252.8612\n",
      "Iter: 373, running avg elbo: -1244.5713\n",
      "Iter: 374, running avg elbo: -1236.2950\n",
      "Iter: 375, running avg elbo: -1227.6323\n",
      "Iter: 376, running avg elbo: -1218.9235\n",
      "Iter: 377, running avg elbo: -1210.3516\n",
      "Iter: 378, running avg elbo: -1201.9054\n",
      "Iter: 379, running avg elbo: -1193.5370\n",
      "Iter: 380, running avg elbo: -1185.1380\n",
      "Iter: 381, running avg elbo: -1176.7744\n",
      "Iter: 382, running avg elbo: -1179.2770\n",
      "Iter: 383, running avg elbo: -1185.2178\n",
      "Iter: 384, running avg elbo: -1190.2148\n",
      "Iter: 385, running avg elbo: -1196.7294\n",
      "Iter: 386, running avg elbo: -1201.8950\n",
      "Iter: 387, running avg elbo: -1205.3485\n",
      "Iter: 388, running avg elbo: -1209.2087\n",
      "Iter: 389, running avg elbo: -1210.1263\n",
      "Iter: 390, running avg elbo: -1212.3668\n",
      "Iter: 391, running avg elbo: -1215.1005\n",
      "Iter: 392, running avg elbo: -1214.9213\n",
      "Iter: 393, running avg elbo: -1213.8940\n",
      "Iter: 394, running avg elbo: -1212.5382\n",
      "Iter: 395, running avg elbo: -1210.0421\n",
      "Iter: 396, running avg elbo: -1208.3176\n",
      "Iter: 397, running avg elbo: -1205.3878\n",
      "Iter: 398, running avg elbo: -1202.2164\n",
      "Iter: 399, running avg elbo: -1198.9288\n",
      "Iter: 400, running avg elbo: -1194.8372\n",
      "Iter: 401, running avg elbo: -1191.3914\n",
      "Iter: 402, running avg elbo: -1187.2178\n",
      "Iter: 403, running avg elbo: -1182.4719\n",
      "Iter: 404, running avg elbo: -1177.4422\n",
      "Iter: 405, running avg elbo: -1172.6947\n",
      "Iter: 406, running avg elbo: -1167.6483\n",
      "Iter: 407, running avg elbo: -1162.2455\n",
      "Iter: 408, running avg elbo: -1156.8012\n",
      "Iter: 409, running avg elbo: -1151.0556\n",
      "Iter: 410, running avg elbo: -1145.5951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 411, running avg elbo: -1139.8484\n",
      "Iter: 412, running avg elbo: -1134.0622\n",
      "Iter: 413, running avg elbo: -1128.2492\n",
      "Iter: 414, running avg elbo: -1122.2345\n",
      "Iter: 415, running avg elbo: -1116.4433\n",
      "Iter: 416, running avg elbo: -1110.4264\n",
      "Iter: 417, running avg elbo: -1104.5067\n",
      "Iter: 418, running avg elbo: -1098.4292\n",
      "Iter: 419, running avg elbo: -1092.2987\n",
      "Iter: 420, running avg elbo: -1086.2466\n",
      "Iter: 421, running avg elbo: -1080.1567\n",
      "Iter: 422, running avg elbo: -1074.1378\n",
      "Iter: 423, running avg elbo: -1067.9998\n",
      "Iter: 424, running avg elbo: -1061.8541\n",
      "Iter: 425, running avg elbo: -1055.7872\n",
      "Iter: 426, running avg elbo: -1049.6501\n",
      "Iter: 427, running avg elbo: -1043.5607\n",
      "Iter: 428, running avg elbo: -1037.4480\n",
      "Iter: 429, running avg elbo: -1031.4577\n",
      "Iter: 430, running avg elbo: -1025.4141\n",
      "Iter: 431, running avg elbo: -1019.4057\n",
      "Iter: 432, running avg elbo: -1013.4099\n",
      "Iter: 433, running avg elbo: -1007.4094\n",
      "Iter: 434, running avg elbo: -1001.5220\n",
      "Iter: 435, running avg elbo: -995.5683\n",
      "Iter: 436, running avg elbo: -989.5875\n",
      "Iter: 437, running avg elbo: -983.5603\n",
      "Iter: 438, running avg elbo: -977.6833\n",
      "Iter: 439, running avg elbo: -971.7971\n",
      "Iter: 440, running avg elbo: -965.9187\n",
      "Iter: 441, running avg elbo: -960.1034\n",
      "Iter: 442, running avg elbo: -954.3129\n",
      "Iter: 443, running avg elbo: -948.5315\n",
      "Iter: 444, running avg elbo: -942.7986\n",
      "Iter: 445, running avg elbo: -937.0727\n",
      "Iter: 446, running avg elbo: -931.3876\n",
      "Iter: 447, running avg elbo: -925.7530\n",
      "Iter: 448, running avg elbo: -920.1571\n",
      "Iter: 449, running avg elbo: -914.5860\n",
      "Iter: 450, running avg elbo: -909.0351\n",
      "Iter: 451, running avg elbo: -903.4295\n",
      "Iter: 452, running avg elbo: -897.9150\n",
      "Iter: 453, running avg elbo: -892.3845\n",
      "Iter: 454, running avg elbo: -886.9006\n",
      "Iter: 455, running avg elbo: -881.4492\n",
      "Iter: 456, running avg elbo: -876.0903\n",
      "Iter: 457, running avg elbo: -870.7495\n",
      "Iter: 458, running avg elbo: -865.3956\n",
      "Iter: 459, running avg elbo: -860.0874\n",
      "Iter: 460, running avg elbo: -854.8275\n",
      "Iter: 461, running avg elbo: -849.5305\n",
      "Iter: 462, running avg elbo: -844.3027\n",
      "Iter: 463, running avg elbo: -839.1205\n",
      "Iter: 464, running avg elbo: -833.9340\n",
      "Iter: 465, running avg elbo: -828.8072\n",
      "Iter: 466, running avg elbo: -823.7332\n",
      "Iter: 467, running avg elbo: -818.6593\n",
      "Iter: 468, running avg elbo: -813.5981\n",
      "Iter: 469, running avg elbo: -808.6018\n",
      "Iter: 470, running avg elbo: -803.6516\n",
      "Iter: 471, running avg elbo: -798.6659\n",
      "Iter: 472, running avg elbo: -793.6983\n",
      "Iter: 473, running avg elbo: -788.7750\n",
      "Iter: 474, running avg elbo: -783.9125\n",
      "Iter: 475, running avg elbo: -779.0377\n",
      "Iter: 476, running avg elbo: -774.1587\n",
      "Iter: 477, running avg elbo: -769.3421\n",
      "Iter: 478, running avg elbo: -764.5963\n",
      "Iter: 479, running avg elbo: -759.8284\n",
      "Iter: 480, running avg elbo: -755.1209\n",
      "Iter: 481, running avg elbo: -750.4511\n",
      "Iter: 482, running avg elbo: -745.7652\n",
      "Iter: 483, running avg elbo: -741.1959\n",
      "Iter: 484, running avg elbo: -736.6237\n",
      "Iter: 485, running avg elbo: -732.0784\n",
      "Iter: 486, running avg elbo: -727.5180\n",
      "Iter: 487, running avg elbo: -723.0309\n",
      "Iter: 488, running avg elbo: -718.4993\n",
      "Iter: 489, running avg elbo: -714.0674\n",
      "Iter: 490, running avg elbo: -709.6586\n",
      "Iter: 491, running avg elbo: -705.2599\n",
      "Iter: 492, running avg elbo: -700.9058\n",
      "Iter: 493, running avg elbo: -696.5672\n",
      "Iter: 494, running avg elbo: -692.2636\n",
      "Iter: 495, running avg elbo: -687.9810\n",
      "Iter: 496, running avg elbo: -683.7063\n",
      "Iter: 497, running avg elbo: -679.4996\n",
      "Iter: 498, running avg elbo: -675.3072\n",
      "Iter: 499, running avg elbo: -671.1883\n",
      "Iter: 500, running avg elbo: -667.0611\n",
      "Iter: 501, running avg elbo: -662.9664\n",
      "Iter: 502, running avg elbo: -658.9161\n",
      "Iter: 503, running avg elbo: -654.8867\n",
      "Iter: 504, running avg elbo: -650.8453\n",
      "Iter: 505, running avg elbo: -646.9083\n",
      "Iter: 506, running avg elbo: -643.0146\n",
      "Iter: 507, running avg elbo: -639.1188\n",
      "Iter: 508, running avg elbo: -635.2369\n",
      "Iter: 509, running avg elbo: -631.3634\n",
      "Iter: 510, running avg elbo: -627.5634\n",
      "Iter: 511, running avg elbo: -623.7894\n",
      "Iter: 512, running avg elbo: -620.0062\n",
      "Iter: 513, running avg elbo: -616.2876\n",
      "Iter: 514, running avg elbo: -612.5742\n",
      "Iter: 515, running avg elbo: -608.8826\n",
      "Iter: 516, running avg elbo: -605.1999\n",
      "Iter: 517, running avg elbo: -601.5717\n",
      "Iter: 518, running avg elbo: -597.9401\n",
      "Iter: 519, running avg elbo: -594.3543\n",
      "Iter: 520, running avg elbo: -590.8063\n",
      "Iter: 521, running avg elbo: -587.2439\n",
      "Iter: 522, running avg elbo: -583.7468\n",
      "Iter: 523, running avg elbo: -580.2239\n",
      "Iter: 524, running avg elbo: -576.7085\n",
      "Iter: 525, running avg elbo: -573.2407\n",
      "Iter: 526, running avg elbo: -569.8563\n",
      "Iter: 527, running avg elbo: -566.4680\n",
      "Iter: 528, running avg elbo: -563.0759\n",
      "Iter: 529, running avg elbo: -559.7393\n",
      "Iter: 530, running avg elbo: -556.4688\n",
      "Iter: 531, running avg elbo: -553.1635\n",
      "Iter: 532, running avg elbo: -549.8875\n",
      "Iter: 533, running avg elbo: -546.6314\n",
      "Iter: 534, running avg elbo: -543.4161\n",
      "Iter: 535, running avg elbo: -540.2353\n",
      "Iter: 536, running avg elbo: -537.0708\n",
      "Iter: 537, running avg elbo: -533.9171\n",
      "Iter: 538, running avg elbo: -530.8325\n",
      "Iter: 539, running avg elbo: -527.7468\n",
      "Iter: 540, running avg elbo: -524.6733\n",
      "Iter: 541, running avg elbo: -521.6563\n",
      "Iter: 542, running avg elbo: -518.6365\n",
      "Iter: 543, running avg elbo: -515.6498\n",
      "Iter: 544, running avg elbo: -512.7192\n",
      "Iter: 545, running avg elbo: -509.7529\n",
      "Iter: 546, running avg elbo: -506.8343\n",
      "Iter: 547, running avg elbo: -503.9442\n",
      "Iter: 548, running avg elbo: -501.0572\n",
      "Iter: 549, running avg elbo: -498.2108\n",
      "Iter: 550, running avg elbo: -495.3821\n",
      "Iter: 551, running avg elbo: -492.5686\n",
      "Iter: 552, running avg elbo: -489.7592\n",
      "Iter: 553, running avg elbo: -487.0003\n",
      "Iter: 554, running avg elbo: -484.2510\n",
      "Iter: 555, running avg elbo: -481.5064\n",
      "Iter: 556, running avg elbo: -478.7904\n",
      "Iter: 557, running avg elbo: -476.1169\n",
      "Iter: 558, running avg elbo: -473.4406\n",
      "Iter: 559, running avg elbo: -470.7906\n",
      "Iter: 560, running avg elbo: -468.1724\n",
      "Iter: 561, running avg elbo: -465.6015\n",
      "Iter: 562, running avg elbo: -463.0265\n",
      "Iter: 563, running avg elbo: -460.5036\n",
      "Iter: 564, running avg elbo: -457.9959\n",
      "Iter: 565, running avg elbo: -455.4993\n",
      "Iter: 566, running avg elbo: -453.0053\n",
      "Iter: 567, running avg elbo: -450.6421\n",
      "Iter: 568, running avg elbo: -448.2818\n",
      "Iter: 569, running avg elbo: -445.8738\n",
      "Iter: 570, running avg elbo: -443.4893\n",
      "Iter: 571, running avg elbo: -441.1175\n",
      "Iter: 572, running avg elbo: -438.7434\n",
      "Iter: 573, running avg elbo: -436.3916\n",
      "Iter: 574, running avg elbo: -434.0722\n",
      "Iter: 575, running avg elbo: -431.7816\n",
      "Iter: 576, running avg elbo: -429.4597\n",
      "Iter: 577, running avg elbo: -427.2139\n",
      "Iter: 578, running avg elbo: -424.9760\n",
      "Iter: 579, running avg elbo: -422.7386\n",
      "Iter: 580, running avg elbo: -420.4979\n",
      "Iter: 581, running avg elbo: -418.2981\n",
      "Iter: 582, running avg elbo: -416.1374\n",
      "Iter: 583, running avg elbo: -414.0049\n",
      "Iter: 584, running avg elbo: -411.8849\n",
      "Iter: 585, running avg elbo: -409.7480\n",
      "Iter: 586, running avg elbo: -407.6541\n",
      "Iter: 587, running avg elbo: -405.5528\n",
      "Iter: 588, running avg elbo: -403.4722\n",
      "Iter: 589, running avg elbo: -401.4155\n",
      "Iter: 590, running avg elbo: -399.4027\n",
      "Iter: 591, running avg elbo: -397.4285\n",
      "Iter: 592, running avg elbo: -395.4393\n",
      "Iter: 593, running avg elbo: -393.5078\n",
      "Iter: 594, running avg elbo: -391.5544\n",
      "Iter: 595, running avg elbo: -389.6635\n",
      "Iter: 596, running avg elbo: -387.7472\n",
      "Iter: 597, running avg elbo: -385.8361\n",
      "Iter: 598, running avg elbo: -383.9469\n",
      "Iter: 599, running avg elbo: -382.1005\n",
      "Iter: 600, running avg elbo: -380.2442\n",
      "Iter: 601, running avg elbo: -378.3778\n",
      "Iter: 602, running avg elbo: -376.5868\n",
      "Iter: 603, running avg elbo: -374.7936\n",
      "Iter: 604, running avg elbo: -372.9781\n",
      "Iter: 605, running avg elbo: -371.1886\n",
      "Iter: 606, running avg elbo: -369.4084\n",
      "Iter: 607, running avg elbo: -367.6355\n",
      "Iter: 608, running avg elbo: -365.8741\n",
      "Iter: 609, running avg elbo: -364.1324\n",
      "Iter: 610, running avg elbo: -362.3991\n",
      "Iter: 611, running avg elbo: -360.7057\n",
      "Iter: 612, running avg elbo: -359.0236\n",
      "Iter: 613, running avg elbo: -357.3717\n",
      "Iter: 614, running avg elbo: -355.7125\n",
      "Iter: 615, running avg elbo: -354.0821\n",
      "Iter: 616, running avg elbo: -352.4528\n",
      "Iter: 617, running avg elbo: -350.8082\n",
      "Iter: 618, running avg elbo: -349.1911\n",
      "Iter: 619, running avg elbo: -347.6544\n",
      "Iter: 620, running avg elbo: -346.1465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 621, running avg elbo: -344.6001\n",
      "Iter: 622, running avg elbo: -343.0117\n",
      "Iter: 623, running avg elbo: -341.4798\n",
      "Iter: 624, running avg elbo: -339.9961\n",
      "Iter: 625, running avg elbo: -338.5031\n",
      "Iter: 626, running avg elbo: -336.9809\n",
      "Iter: 627, running avg elbo: -335.4987\n",
      "Iter: 628, running avg elbo: -334.0395\n",
      "Iter: 629, running avg elbo: -332.5678\n",
      "Iter: 630, running avg elbo: -331.0872\n",
      "Iter: 631, running avg elbo: -329.6462\n",
      "Iter: 632, running avg elbo: -328.2422\n",
      "Iter: 633, running avg elbo: -326.8182\n",
      "Iter: 634, running avg elbo: -325.4018\n",
      "Iter: 635, running avg elbo: -324.0015\n",
      "Iter: 636, running avg elbo: -322.6566\n",
      "Iter: 637, running avg elbo: -321.3211\n",
      "Iter: 638, running avg elbo: -319.9452\n",
      "Iter: 639, running avg elbo: -318.5721\n",
      "Iter: 640, running avg elbo: -317.2554\n",
      "Iter: 641, running avg elbo: -315.9650\n",
      "Iter: 642, running avg elbo: -314.6606\n",
      "Iter: 643, running avg elbo: -313.3650\n",
      "Iter: 644, running avg elbo: -312.0718\n",
      "Iter: 645, running avg elbo: -310.8065\n",
      "Iter: 646, running avg elbo: -309.5377\n",
      "Iter: 647, running avg elbo: -308.2570\n",
      "Iter: 648, running avg elbo: -307.0163\n",
      "Iter: 649, running avg elbo: -305.8041\n",
      "Iter: 650, running avg elbo: -304.6079\n",
      "Iter: 651, running avg elbo: -303.3818\n",
      "Iter: 652, running avg elbo: -302.1641\n",
      "Iter: 653, running avg elbo: -300.9436\n",
      "Iter: 654, running avg elbo: -299.7423\n",
      "Iter: 655, running avg elbo: -298.5774\n",
      "Iter: 656, running avg elbo: -297.4048\n",
      "Iter: 657, running avg elbo: -296.2416\n",
      "Iter: 658, running avg elbo: -295.0661\n",
      "Iter: 659, running avg elbo: -293.9147\n",
      "Iter: 660, running avg elbo: -292.8042\n",
      "Iter: 661, running avg elbo: -291.6751\n",
      "Iter: 662, running avg elbo: -290.5538\n",
      "Iter: 663, running avg elbo: -289.4469\n",
      "Iter: 664, running avg elbo: -288.3385\n",
      "Iter: 665, running avg elbo: -287.2350\n",
      "Iter: 666, running avg elbo: -286.1353\n",
      "Iter: 667, running avg elbo: -285.0438\n",
      "Iter: 668, running avg elbo: -283.9550\n",
      "Iter: 669, running avg elbo: -282.8846\n",
      "Iter: 670, running avg elbo: -281.8099\n",
      "Iter: 671, running avg elbo: -280.7472\n",
      "Iter: 672, running avg elbo: -279.6964\n",
      "Iter: 673, running avg elbo: -278.6451\n",
      "Iter: 674, running avg elbo: -277.6207\n",
      "Iter: 675, running avg elbo: -276.5979\n",
      "Iter: 676, running avg elbo: -275.5894\n",
      "Iter: 677, running avg elbo: -274.5805\n",
      "Iter: 678, running avg elbo: -273.6058\n",
      "Iter: 679, running avg elbo: -272.6270\n",
      "Iter: 680, running avg elbo: -271.6589\n",
      "Iter: 681, running avg elbo: -270.6746\n",
      "Iter: 682, running avg elbo: -269.7281\n",
      "Iter: 683, running avg elbo: -268.7806\n",
      "Iter: 684, running avg elbo: -267.8425\n",
      "Iter: 685, running avg elbo: -266.9147\n",
      "Iter: 686, running avg elbo: -265.9878\n",
      "Iter: 687, running avg elbo: -265.0547\n",
      "Iter: 688, running avg elbo: -264.1285\n",
      "Iter: 689, running avg elbo: -263.2207\n",
      "Iter: 690, running avg elbo: -262.3294\n",
      "Iter: 691, running avg elbo: -261.4550\n",
      "Iter: 692, running avg elbo: -260.5546\n",
      "Iter: 693, running avg elbo: -259.6950\n",
      "Iter: 694, running avg elbo: -258.8336\n",
      "Iter: 695, running avg elbo: -257.9604\n",
      "Iter: 696, running avg elbo: -257.1154\n",
      "Iter: 697, running avg elbo: -256.2558\n",
      "Iter: 698, running avg elbo: -255.4075\n",
      "Iter: 699, running avg elbo: -254.5895\n",
      "Iter: 700, running avg elbo: -253.9047\n",
      "Iter: 701, running avg elbo: -253.5283\n",
      "Iter: 702, running avg elbo: -254.8312\n",
      "Iter: 703, running avg elbo: -255.6448\n",
      "Iter: 704, running avg elbo: -256.3494\n",
      "Iter: 705, running avg elbo: -256.2871\n",
      "Iter: 706, running avg elbo: -256.5463\n",
      "Iter: 707, running avg elbo: -256.8582\n",
      "Iter: 708, running avg elbo: -256.9299\n",
      "Iter: 709, running avg elbo: -257.0547\n",
      "Iter: 710, running avg elbo: -256.9648\n",
      "Iter: 711, running avg elbo: -256.9678\n",
      "Iter: 712, running avg elbo: -256.8611\n",
      "Iter: 713, running avg elbo: -256.6616\n",
      "Iter: 714, running avg elbo: -256.4096\n",
      "Iter: 715, running avg elbo: -256.2040\n",
      "Iter: 716, running avg elbo: -255.9782\n",
      "Iter: 717, running avg elbo: -255.8202\n",
      "Iter: 718, running avg elbo: -255.4261\n",
      "Iter: 719, running avg elbo: -255.0725\n",
      "Iter: 720, running avg elbo: -254.7186\n",
      "Iter: 721, running avg elbo: -254.2618\n",
      "Iter: 722, running avg elbo: -253.9146\n",
      "Iter: 723, running avg elbo: -253.3938\n",
      "Iter: 724, running avg elbo: -252.9592\n",
      "Iter: 725, running avg elbo: -252.4958\n",
      "Iter: 726, running avg elbo: -251.9332\n",
      "Iter: 727, running avg elbo: -251.4089\n",
      "Iter: 728, running avg elbo: -250.9054\n",
      "Iter: 729, running avg elbo: -250.3884\n",
      "Iter: 730, running avg elbo: -249.8553\n",
      "Iter: 731, running avg elbo: -249.2478\n",
      "Iter: 732, running avg elbo: -248.7142\n",
      "Iter: 733, running avg elbo: -248.2041\n",
      "Iter: 734, running avg elbo: -247.6609\n",
      "Iter: 735, running avg elbo: -247.0723\n",
      "Iter: 736, running avg elbo: -246.4757\n",
      "Iter: 737, running avg elbo: -245.9017\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for itr in range(1, niters + 1):\n",
    "        optimizer.zero_grad()\n",
    "        # backward in time to infer q(z_0)\n",
    "        h = rec.initHidden().to(device)\n",
    "        for t in reversed(range(samp_trajs.size(1))):\n",
    "            obs = samp_trajs[:, t, :]\n",
    "            out, h = rec.forward(obs, h)\n",
    "        qz0_mean, qz0_logvar = out[:, :latent_dim], out[:, latent_dim:]\n",
    "        epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "        z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "        # forward in time and solve ode for reconstructions\n",
    "        pred_z = odeint(func, z0, samp_ts).permute(1, 0, 2)\n",
    "        pred_x = dec(pred_z)\n",
    "\n",
    "        # compute loss\n",
    "        noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
    "        noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
    "        logpx = log_normal_pdf(\n",
    "            samp_trajs, pred_x, noise_logvar).sum(-1).sum(-1)\n",
    "        pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
    "        analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
    "                                pz0_mean, pz0_logvar).sum(-1)\n",
    "        loss = torch.mean(-logpx + analytic_kl, dim=0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_meter.update(loss.item())\n",
    "\n",
    "        print('Iter: {}, running avg elbo: {:.4f}'.format(itr, -loss_meter.avg))\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    if train_dir is not None:\n",
    "        ckpt_path = os.path.join(train_dir, 'ckpt.pth')\n",
    "        torch.save({\n",
    "            'func_state_dict': func.state_dict(),\n",
    "            'rec_state_dict': rec.state_dict(),\n",
    "            'dec_state_dict': dec.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'orig_trajs': orig_trajs,\n",
    "            'samp_trajs': samp_trajs,\n",
    "            'orig_ts': orig_ts,\n",
    "            'samp_ts': samp_ts,\n",
    "        }, ckpt_path)\n",
    "        print('Stored ckpt at {}'.format(ckpt_path))\n",
    "print('Training complete after {} iters.'.format(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # sample from trajectorys' approx. posterior\n",
    "    h = rec.initHidden().to(device)\n",
    "    for t in reversed(range(samp_trajs.size(1))):\n",
    "        obs = samp_trajs[:, t, :]\n",
    "        out, h = rec.forward(obs, h)\n",
    "    qz0_mean, qz0_logvar = out[:, :latent_dim], out[:, latent_dim:]\n",
    "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "    orig_ts = torch.from_numpy(orig_ts).float().to(device)\n",
    "\n",
    "    # take first trajectory for visualization\n",
    "    z0 = z0[0]\n",
    "\n",
    "    ts_pos = np.linspace(0., 2. * np.pi, num=2000)\n",
    "    ts_neg = np.linspace(-np.pi, 0., num=2000)[::-1].copy()\n",
    "    ts_pos = torch.from_numpy(ts_pos).float().to(device)\n",
    "    ts_neg = torch.from_numpy(ts_neg).float().to(device)\n",
    "\n",
    "    zs_pos = odeint(func, z0, ts_pos)\n",
    "    zs_neg = odeint(func, z0, ts_neg)\n",
    "\n",
    "    xs_pos = dec(zs_pos)\n",
    "    xs_neg = torch.flip(dec(zs_neg), dims=[0])\n",
    "\n",
    "xs_pos = xs_pos.cpu().numpy()\n",
    "xs_neg = xs_neg.cpu().numpy()\n",
    "orig_traj = orig_trajs[0].cpu().numpy()\n",
    "samp_traj = samp_trajs[0].cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(orig_traj[:, 0], orig_traj[:, 1],\n",
    "         'g', label='true trajectory')\n",
    "plt.plot(xs_pos[:, 0], xs_pos[:, 1], 'r',\n",
    "         label='learned trajectory (t>0)')\n",
    "plt.plot(xs_neg[:, 0], xs_neg[:, 1], 'c',\n",
    "         label='learned trajectory (t<0)')\n",
    "plt.scatter(samp_traj[:, 0], samp_traj[\n",
    "            :, 1], label='sampled data', s=3)\n",
    "plt.legend()\n",
    "plt.savefig('./vis.png', dpi=500)\n",
    "print('Saved visualization figure at {}'.format('./vis.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = zs_pos.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts_pos.cpu().numpy(),z[:,0])\n",
    "plt.plot(ts_pos.cpu().numpy(),z[:,1])\n",
    "plt.plot(ts_pos.cpu().numpy(),z[:,2])\n",
    "plt.plot(ts_pos.cpu().numpy(),z[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
